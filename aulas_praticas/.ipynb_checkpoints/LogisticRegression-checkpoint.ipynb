{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressão logística é uma técnica de classificação que pertence ao grupo dos classificadores lineares* e de alguma forma parecida com regressão linear. É uma técnica rápida e simples de entender, além de conveniente para interpretar os resultados. Apesar de ser, essencialmente, um método para classificação binária, pode ser usado para classificação multi-classes.\n",
    "\n",
    "\\* classificação baseada em uma combinação linear entre as variáveis de entrada, também chamado de _logit_.\n",
    "\n",
    "ex: $f(X) = \\alpha + b_1x_1 + b_2x_2 +\\dots + b_nx_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função Sigmoid\n",
    "\n",
    "O objetivo da Regressão Logística é encontrar a função $p($**x**$)$ tal que as saídas estimadas $p(x_i)$ fiquem o mais próximo possível do valor real de $y_i$ para cada amostra $i = 1, 2, \\dots, n$. Como, em um primeiro momento, vamos lidar exclusivamente de classificação binária, essas saídas devem ser apenas $0$ ou $1$. Sendo assim, é conveniente utilizar a função Sigmoid (caso especial da função logística).\n",
    "\n",
    "<img src=\"assets/log-reg-1.webp\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntando a combinação linear com a função Sigmoid\n",
    "\n",
    "A Regressão logística determina os melhores valores para o bias $\\alpha$ e os pesos $b_1, b_2, \\dots, b_n$, tal que a função $p($**x**$)$ se aproxime da classe real da amostra. O processo de encontrar os melhores valores para esses parâmetros é chamado de treinamento (ou _fitting_ ).\n",
    "\n",
    "Nesse caso, **x** que a função Sigmoid ($p$) recebe é a saída da combinação linear $f(X) = \\alpha + b_1x_1 + b_2x_2 +\\dots + b_nx_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como encontrar os melhores valores para o bias e os pesos?\n",
    "\n",
    "Os melhores pesos são geralmente encontrados maximizando a função de verosimilhança logarítmica (_log-likelihood_ ) O método é chamado de maximização da verossimilhança (_maximum likelihood estimation_ - MLE) e é representado pela seguinte equação.\n",
    "\n",
    "\\begin{equation}\n",
    "MLE = \\sum_{i=1}^n(y_i log(p(x_i)) + (1 − y_i) log(1 − p(x_i))).\n",
    "\\end{equation}\n",
    "<img src=\"assets/log-reg-4.webp\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Os melhores parâmetros para maximizar essa equação podem ser encontrados usando técnicas matemáticas, essa parte será executada pelo scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação\n",
    "\n",
    "### Exemplo 1 - Classificação binária com uma única variável de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo n° 1: Importando as bibliotecas necessárias\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [9]]\n",
      "y = [0 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Passo n° 2: Gerando dados para classificação\n",
    "\n",
    "# Gerando os dados de entrada. Serão geradas 10 amostras com valores de 0 a 9\n",
    "# note que os modelos do scikit-learn esperam um array composto com um array de características\n",
    "# por amostra de entrada (mesmo que seja apenas 1 característica). Por isso fazemos um reshape.\n",
    "x = np.arange(10).reshape(-1, 1) \n",
    "\n",
    "# gerando as classes para cada amostra\n",
    "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "print('x = {}'.format(x))\n",
    "print('y = {}'.format(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passo n° 3: Criando e treinando um modelo\n",
    "\n",
    "# Criando ou estanciando o modelo:\n",
    "#   solver é o algoritmo utilizado para otimizar os parâmetros.\n",
    "#     liblinear é a melhor opção para datasets pequenos e binários.\n",
    "#   random_state = 0, ou qualquer número, garante que a inicialização aleatória vai ser sempre igual, \n",
    "#     independente de quantas vezes rodar o algoritmo\n",
    "model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "\n",
    "# O modelo possui diversos hiperparâmetros que podem ser ajustados, como penalidade (padrão L2), número \n",
    "#   de iterações, etc. Sugiro que deem uma olhada na documentação:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes = [0 1]\n",
      "bias = [-1.04608067]\n",
      "pesos = [[0.51491375]]\n"
     ]
    }
   ],
   "source": [
    "# Podemos extrair algumas informações do modelo, como por exemplo:\n",
    "\n",
    "# as classes presentes no conjunto de dados\n",
    "print('classes = {}'.format(model.classes_))\n",
    "\n",
    "# o valor do bias\n",
    "print('bias = {}'.format(model.intercept_))\n",
    "\n",
    "# os pesos\n",
    "print('pesos = {}'.format(model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilidades = [[0.74002157 0.25997843]\n",
      " [0.62975524 0.37024476]\n",
      " [0.5040632  0.4959368 ]\n",
      " [0.37785549 0.62214451]\n",
      " [0.26628093 0.73371907]\n",
      " [0.17821501 0.82178499]\n",
      " [0.11472079 0.88527921]\n",
      " [0.07186982 0.92813018]\n",
      " [0.04422513 0.95577487]\n",
      " [0.02690569 0.97309431]]\n",
      "\n",
      "classe_estimada = [0 0 0 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Passo n° 4: Avaliando o modelo\n",
    "\n",
    "# uma das formas de avaliar o modelo, é através a probabilidade  que ele dá à cada amostra\n",
    "#   de pertencer a cada classe\n",
    "probabilidades = model.predict_proba(x)\n",
    "\n",
    "# uma outra forma mais comum é através da classe que o modelo estimou\n",
    "classe_estimada = model.predict(x)\n",
    "\n",
    "print('probabilidades = {}\\n'.format(probabilidades))\n",
    "print('classe_estimada = {}'.format(classe_estimada))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar essa classificação na figura a seguir:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('torchEnv': conda)",
   "language": "python",
   "name": "python37764bittorchenvconda44a76b6c90fa48eea1939354f840259c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
